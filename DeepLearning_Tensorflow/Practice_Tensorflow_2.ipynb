{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plt_dynamic(epoch_list, avg_train_loss_list, avg_test_loss_list, ax,ticks,title):\n",
    "    ax.plot(epoch_list, avg_train_loss_list, 'b', label=\"Train Loss\")\n",
    "    ax.plot(epoch_list, avg_test_loss_list, 'r', label=\"Test Loss\")\n",
    "    if len(epoch_list)==1:\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "    plt.yticks(ticks)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 512 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for mini-batch dataset\n",
    "x_is = tf.placeholder(tf.float32, [None, 784])\n",
    "y_true_is = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would be used for dropouts in dropouts section\n",
    "\n",
    "# keep_prob: we will be using these placeholders when we use dropouts, while testing model\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# keep_prob_input: we will be using these placeholders when we use dropouts, while training model\n",
    "keep_prob_input = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "\n",
    "# sgd means sigmoidal unit. So this is the initialization for sigmoidal activation units\n",
    "# SGD: Xavier/Glorot Normal initialization.\n",
    "weights_sgd = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 \n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "\n",
    "# for relu activation unit\n",
    "# He Normal initialization.\n",
    "weights_relu = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.0505, mean=0)),    #784x512\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.0624, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.125, mean=0))  #128x10\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 15\n",
    "learning_rate = 0.001  # this is value of eeettaa i.e if we use SGD optimiser then this would be fixed.\n",
    "batch_size = 100 # mini-batch size for each iteration.\n",
    "display_step = 1 # it is to tell, after how many epochs interval, you want to update the graph of loss Vs epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>STEPS :<br>\n",
    "<p>    \n",
    "   a) Define your y_hat_is in terms of W's , x's and in-between layers by defining your multilayer_perceptron.<br><br>\n",
    "   b) Define your loss function in terms of y_hat_is and y_is.<br><br>\n",
    "   c) Define/Choose the optimiser which would reduce the loss which you defined by improving W's of each layer.<br><br>\n",
    "   d) In order to start training i.e in order to update W's for each layer, you have to pass your optimiser inside the sess.run() method by filling the memory of placeholders i.e by sending mini-batch dataset for x_is and y_true_is.\n",
    "</p>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model 1: input (784) - sigmoid(512) - sigmoid(128) - \n",
    "softmax(output 10) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model (this is the place where you define architecture of layers and activation units in each of those layers)\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Softmax activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.softmax(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> __ Model 1 + AdamOptimizer __ (Sigmoid + Adam) </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (?, 784) W[h1]: (784, 512) b[h1]: (512,)\n",
      "layer_1: (?, 512) W[h2]: (512, 128) b[h2]: (128,)\n",
      "layer_2: (?, 128) W[out]: (128, 10) b3: (10,)\n",
      "out_layer: (?, 10)\n",
      "WARNING:tensorflow:From <ipython-input-45-0a0df2b807a4>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\n",
    "y_hat_sgd = multilayer_perceptron(x_is, weights_sgd, biases) # this is final y_hat_is.\n",
    "\n",
    "# this is loss function for sigmoid activation units, in hidden layers.\n",
    "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_hat_sgd, labels = y_true_is))\n",
    "\n",
    "# there are many other optimizers available (these are the optimisers, which would reduce cost_sgd)\n",
    "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "\n",
    "# ? means that you have to pass mini-batch dataset, at the time of actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initializing all the weights and biases i.e of all the layers\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "     # for plotting graph\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    epochs_list, avg_train_loss_list, avg_test_loss_list = [], [], []\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here c: corresponds to the parameter cost_sgd\n",
    "            # w : corresponds to the parameter weights_sgd\n",
    "            # c = sess.run() returns the cost after every batch during train --> this is training error after the iteration\n",
    "            # w = sess.run() returns the weights that are modified after every batch through Back prop\n",
    "            # w is dict w = {'h1': updated h1 weight vector after the current batch,\n",
    "            #                'h2': updated h2 weight vector after the current batch, \n",
    "            #                'out': updated output weight vector after the current batch, \n",
    "            #                }\n",
    "            # you check these w matrix for every iteration, and check whats happening during back prop\n",
    "            #\n",
    "            # note: sess.run() returns parameter values based on the input parameters\n",
    "            # feed_dict={x_is: batch_xs, y_true_is: batch_ys} here x_is, y_true_is should be placeholders\n",
    "            # x_is, y_true_is are the input parameters on which the model gets trained.\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x_is: batch_xs, y_true_is: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x_is: mnist.test.images, y_true_is: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        epochs_list.append(epoch)\n",
    "        avg_train_loss_list.append(train_avg_cost)\n",
    "        avg_test_loss_list.append(test_avg_cost)\n",
    "        plt_dynamic(epochs_list, avg_train_loss_list, avg_test_loss_list, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
